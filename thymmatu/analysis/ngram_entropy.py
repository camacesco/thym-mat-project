#!/usr/bin/env python
# -*- coding: utf-8 -*-

'''
    Easy Plot
    Copyright (C) November 2021 Francesco Camaglia, LPENS 

    WARNING: 
    As it is, the program works only on files with the format of generated olga files.
'''

#####################
#  LOADING MODULES  #
#####################

import os, sys
import pandas as pd
import numpy as np
import optparse
from tqdm import tqdm 
import multiprocessing

from thymmatu.handle import ngrams
from thymmatu.utils import tryMakeDir, fileScope
from thymmatu.handle.statbiophys import openOlga

##########
#  MAIN  #
##########

def main( ) :
    '''
    Evaluete n-grams entropy. Example of usage:
    > python ngram_entropy.py -n 5 -i file.csv.gz --samples 25   
    '''
    
    parser = optparse.OptionParser( conflict_handler="resolve" )

    #####################
    # MANDATORY OPTIONS #
    #####################
    
    parser.add_option( '-n', action='store', dest='num', type="int", help='The "n-grams" length.' )
    parser.add_option( '-a', '--alphabet', action='store', dest='alph', type="string", default="AA", help="The alphabet of the reads." )
    parser.add_option( '-M', '--max_categories', action='store', dest='max_categ', default=0, type="int", help="The maximum number of sequences considered." )
    parser.add_option( '-i', '--inputfile', action="store", dest = 'inputfile', metavar='PATH/TO/FILE', type="string", help='PATH/TO/FILE where Olga sequences are stored.' )

    ######################
    #  SPECIFIC OPTIONS  #
    ######################

    parser.add_option( '-s', '--skip', action="store", dest="skip", type="int", default=0, help="" )
    parser.add_option( '-B', '--beginning_const', action="store", dest="beg", type="int", default=0, help="" )
    parser.add_option( '-E', '--end_const', action="store", dest="end", type="int", default=0, help="" )
    parser.add_option( '-u','--unit', action='store', dest='unit_usr', type='choice', choices = [ "ln", "log2", "log10" ], default="log2", help="The unit of the logarithm." )
    parser.add_option( '--NSBstdDev', action='store_true', dest='NSBstdDev', default=False, help="Wheter to compute NSB std. deviation." )
    
    ##################
    #  USER OPTIONS  #
    ##################

    parser.add_option( '--samples', action='store', dest='n_subSamples', type="int", default=1, help="Number of subsamples (including all dataset)." )
    parser.add_option( '-o', '--outputfile', action="store", dest='outputfile', metavar='PATH/TO/FILE', type="string", default=None, help='The file where entropy has to be saved.' )
    parser.add_option( '--more_entropies', action='store_true', dest='more_entr', default=False, help="Wheter to compute other entropy estimators alongside with NSB." )

    ######################
    # OPTIONS ASSIGNMENT #
    ######################

    options, args = parser.parse_args()

    
    # >>>>>>>>>>>>>>>>>>>
    #  LOADING OPTIONS  #
    # >>>>>>>>>>>>>>>>>>>
    
    num = options.num
    alph = options.alph
    max_categ = options.max_categ
    inputfile = options.inputfile
    outputfile = options.outputfile
    n_subSamples = options.n_subSamples
    skip = options.skip
    beg = options.beg
    end = options.end
    more_entr = options.more_entr
    unit_usr = options.unit_usr
    NSBstdDev = options.NSBstdDev
    
    # >>>>>>>>>>>>>>>>>>>>>>
    #  OUTPUT DEFINITIONS  #
    # >>>>>>>>>>>>>>>>>>>>>>

    if outputfile is not None:
        # User name  
        # clean from eventual extension
        if len( outputfile.split( '.' ) ) != 1:  
            outputfile = outputfile.split('.')[0]
        # eventually create folder
        if os.path.dirname( outputfile ) != "" :
            FLDR = os.path.dirname( outputfile )
            tryMakeDir( FLDR )
            outputfile = os.path.basename( outputfile )
        else :
            FLDR = "."
    else :
        # Default name  
        if os.path.dirname( inputfile ) != "" : 
            FLDR = os.path.dirname( inputfile )
        else : 
            FLDR = "."
        outputfile = f"n{num}-{alph}-s{skip}-b{beg}e{end}"
    # Output names                                                  
    entropy_file = f"{FLDR}/{outputfile}-entr.csv"
    dict_file = f"{FLDR}/{outputfile}-dic.csv.gz"

    # initilialize entropy file  
    Results = []
    headers = ['K_seq', 'N_ngrams', 'K_obs', 'NSB']
    if NSBstdDev is True : headers = headers + ['NSBstdDev']
    if more_entr is True : headers = headers + ['ML', 'MM', 'CS']
    Results.append( headers )

    ###################
    #  SEQUENCE FILE  #
    ###################                   

    # WARNING!: lack of generality
    df = openOlga( inputfile ).dropna( )
    
    #  subsamples definition
    if max_categ == 0 : max_categ = len(df)
    if n_subSamples == 1 : K_vec = [ max_cat ] # option for 1 single measure 
    else :
        # logarithmic subsample sets 
        K_vec = np.logspace( np.log10( 5e2 ), np.log10( max_categ ), num = n_subSamples ).astype(int)
    
    # check for max_categ greater than dataframe and change subsamples
    if max_categ > len( df ) :
        K_vec = [ i for i in K_vec if i < len(df) ]
        K_vec.append( len(df) )

    #################
    #  USER SUMMAR  #
    #################   

    print("> "+str(num)+"grams entropy computation")
    print("> Alphabet of the sequences: " + ngrams._Alphabet_[alph][1] )
    print("> Maximum number of clonotypes considered: ", max_categ)
    print("> Number of subsamples considered: ", n_subSamples)
    print("> skip: ", skip, "| beginning: ", beg, "| end: ", end)
    print("> Input file: ", inputfile )
    print("> Output file: ", entropy_file )
    print("> Number of threads: ", multiprocessing.cpu_count() )

    ###############
    #  EXECUTION  #
    ###############
    
    for subsamp_indx in tqdm( range( 0, len(K_vec), 1 ) ) :
        
        n_categ = K_vec[ subsamp_indx ]
        df_subsamp = df.sample( n = n_categ )   
        
        # the subsample counts_list and estimators are computed
        Gear = ngrams.ngram_gear( num=num, alph=alph ) # initilzation of counts dict
        Gear.counts_dict_update( df_subsamp["aa"].values, 
                                skip=skip, beg=beg, end=end )
        thisSample = [ n_categ, Gear.experiment.N, Gear.experiment.obs_categ ]
        
        if NSBstdDev == True :
            temp = Gear.experiment.entropy( method="NSB", unit=unit_usr, err=True )
            thisSample.append( temp[0] )
            thisSample.append( temp[1] )
        else :
            thisSample.append( Gear.experiment.entropy( method="NSB", unit=unit_usr, err=False ) )
              
        if more_entr == True :
            thisSample.append( Gear.experiment.entropy( method="ML", unit=unit_usr ) )
            thisSample.append( Gear.experiment.entropy( method="MM", unit=unit_usr ) )
            thisSample.append( Gear.experiment.entropy( method="CS", unit=unit_usr ) )

        Results.append( thisSample )   
  
    # >>>>>>>>>>>>>>
    #   CONCLUSION
    # >>>>>>>>>>>>>>

    # save entropy of subsamples
    pd.DataFrame( Results ).to_csv( entropy_file, sep=",", index=None, header=False )
    # save final dictionary of ngrams count if requested
    Gear.save_file_dict( dict_file )
    
###



################
#  EXECUTABLE  #
################

if __name__ == "__main__" : main( )
